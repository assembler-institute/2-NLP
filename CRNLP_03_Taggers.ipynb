{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63417cc0",
   "metadata": {},
   "source": [
    "# NLP III: Taggers in NLP\n",
    "\n",
    "## Previously in NLP\n",
    "\n",
    "In previous notebooks, we explored Context-Free Grammars (CFG). In essence, with CFG, we have a set of tokens (words) to which we assign specific tags. These tags are then organized and classified based on predefined rules. We've identified that this approach has limitations, especially when encountering words that aren't part of our training set‚Äîthose words simply won't be recognized.\n",
    "\n",
    "A potential solution to this problem is using taggers. With the EAGLES standard, we can develop a tagging system that leverages techniques such as N-Grams, Hidden Markov Models, and others.\n",
    "\n",
    "To kick things off, let's import our go-to library for tokenization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e69c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import UnigramTagger, BigramTagger, TrigramTagger #N-Gram\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "from nltk.chunk.regexp import *\n",
    "\n",
    "# Import your library. Type your code here:\n",
    "# ....................\n",
    "\n",
    "# We don't need to import the following library, do you understand why?\n",
    "# from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9985585",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'El ni√±o tiene unos zapatos'\n",
    "tokens = tokenization.tokenize(phrase)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab6954",
   "metadata": {},
   "source": [
    "Let us train the taggers! \n",
    "\n",
    "I.e., let us create the list of tuples: ('word', 'tag')\n",
    "\n",
    "Tags can be found here:\n",
    "\n",
    "https://www.cs.upc.edu/~nlp/tools/parole-sp.html\n",
    "\n",
    "This page sounds familiar?\n",
    "\n",
    "I've already written the code, so you don't have to suffer =)\n",
    "\n",
    "**But please, understad how it works!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus  = [\n",
    "    [('el', 'tdms0'), ('ni√±o', 'ncms000'), ('tiene', 'vmip3s0'), ('unos', 'di3mp00'), ('zapatos', 'ncmp000')],\n",
    "    [('la', 'tdfs0'), ('abuela', 'ncfs000'), ('cocina', 'vmip3s0'), ('un', 'di3ms00'), ('pastel', 'ncms000')],\n",
    "    [('el', 'tdms0'), ('abuelo', 'ncms000'), ('compra', 'vmip3s0'), ('un', 'di3ms00'), ('bast√≥n', 'ncms000')]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405b5e4",
   "metadata": {},
   "source": [
    "## Tagger training\n",
    "\n",
    "**¬°** üëÅÔ∏è **! Words with or without 'tildes' are different tags!**\n",
    "\n",
    "We've previously discussed how to train taggers, remember?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd1903",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram  = UnigramTagger(train_corpus)\n",
    "bigram   = BigramTagger(train_corpus, backoff=unigram)\n",
    "trigram  = TrigramTagger(train_corpus, backoff=bigram)\n",
    "hmm      = HiddenMarkovModelTagger.train(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c178a",
   "metadata": {},
   "source": [
    "Training is evidently a quick process given the limited amount of data. However, it is nearly useless given the limited amount of data.\n",
    "\n",
    "In any case, let's see how it works in all its glory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'El ni√±o cocina unos bastones'\n",
    "\n",
    "print('Tri: ', trigram.tag(tokenization.tokenize(test_phrase)))\n",
    "print('Hmm: ', hmm.tag(tokenization.tokenize(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e626d",
   "metadata": {},
   "source": [
    "Is the tagg of both correct? Which one performs better? The trigram or the HMM?\n",
    "\n",
    "And what is the HMM?\n",
    "\n",
    "And what about this phrase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ae392",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_test = 'La ni√±a tiene una pelota de baloncesto amarilla'\n",
    "print('Hmm: ', hmm.tag(tokenization.tokenize(frase_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef786ac",
   "metadata": {},
   "source": [
    "**NIIIIIIICE!!!!!**\n",
    "\n",
    "**HEY! NO, WAIT!!!**\n",
    "\n",
    "Do not go too fast!!! This algorithm prefers to put something instead of None.\n",
    "\n",
    "**Let's Break This Down**\n",
    "\n",
    "Algorithms, especially statistical ones, often exhibit a behavior we might call \"pattern mimicry\". They don't truly \"understand\" the nuances of human language. Models like N-grams, HMM, and many others base their decisions on statistical patterns found in the training data, not on genuine comprehension of grammatical or semantic rules. Here's what's happening:\n",
    "\n",
    "1) **Lack of Real Understanding:** The algorithm doesn‚Äôt grasp grammar as we do. It sees sequences, counts occurrences, and predicts based on those counts. It recognizes patterns of position and sequence: which word comes before, which one follows, etc.\n",
    "\n",
    "2) **Preference Over 'None':** These algorithms are averse to producing 'None' as an output. They‚Äôd rather produce a label, even if it's not the most accurate one, based on the patterns they've observed in the training data.\n",
    "\n",
    "3) **Mistakes in Labeling:** As you've rightly noticed (or not?), errors crop up: ('una', 'di3ms00') should indeed be ('una', 'di3fs00') to represent the feminine form.\n",
    "Similarly, ('pelota', 'ncms000') should be ('pelota', 'ncfs000') since \"pelota\" is feminine.\n",
    "\n",
    "And if you look carefully enough, in the rest, ('de', 'vmip3s0'), ('baloncesto', 'di3mp00'), ('amarilla', 'ncmp000'), is just repeaging the structure that it learns when training: vmi, di, nc. It does not hit any anyone :(\n",
    "\n",
    "4) **Repetition of Training Data:** If your model seems to be just repeating structures it learned during training without applying them correctly to new data, it's a sign that it may be overfitting to the training data and not generalizing well to new examples.\n",
    "\n",
    "The algorithm is just repeating the structure of the training set üßê   \n",
    "\n",
    "\n",
    "In essence, while these models are powerful and can be quite accurate with enough diverse data, they have limitations. They're excellent pattern recognizers but not linguistic experts. Always review the outputs to understand both the strengths and limitations of these tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7000107",
   "metadata": {},
   "source": [
    "## Training a Tagger\n",
    "\n",
    "Alright, let's dive a little deeper into the world of taggers. We've seen how they can sometimes produce quirky results, right? Now, let's see if we can train one to behave a bit more to our liking.\n",
    "\n",
    "To train a tagger, we essentially need to \"teach\" it about the relationship between words and their corresponding grammatical tags. Think of it like showing a child pictures of different animals and saying, \"This is a dog,\" \"This is a cat,\" and so on. Over time, the child (or in our case, the tagger) learns to recognize and label these animals (or words) on their own.\n",
    "\n",
    "To do this, we utilize a tagged corpus. A corpus is a large and structured set of texts, and a tagged corpus is one where each word in the text is paired with its grammatical tag.\n",
    "\n",
    "**Why is it Important?:** By examining a tagged corpus, our tagger can start to identify patterns, like \"Hey, the word 'runs' is often tagged as a verb.\" Over time, and with enough examples, it can start making educated guesses about words it hasn't seen before.\n",
    "\n",
    "Let's now peek into the English tagged corpus to understand its contents and structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89613ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "print(cess_esp.sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da605b3",
   "metadata": {},
   "source": [
    "As we can see, with the command `cess_esp.sents()` we get a set of tokenised phrases of different themes.\n",
    "\n",
    "And with the command `cess_esp.tagged_sents()` we get the same set of tagged sentences.\n",
    "\n",
    "Can you please characterize the tagged corpus structure?\n",
    "\n",
    "- Count the words, sentences, etc. \n",
    "- Come on, little data scientist! Explore data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9422f96",
   "metadata": {},
   "source": [
    "**Easy excercise:** Merge your original corpus (`train_corpus`), with the tagged corpus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458708d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7bc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2936cd",
   "metadata": {},
   "source": [
    "And train again with this bigger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32addbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c090ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'La ni√±a tiene una pelota de baloncesto amarilla'\n",
    "\n",
    "print('Tri: ', trigram.tag(tokenization.tokenize(test_phrase)))\n",
    "print('Hmm: ', hmm.tag(tokenization.tokenize(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915cf6a",
   "metadata": {},
   "source": [
    "You might observe a discrepancy in the tags assigned to 'la'. The Trigram tagger assigns 'tdfs0', while the HMM tagger assigns 'da0fs0'. This discrepancy arises because we trained the Trigram with the 'tdfs0' tag.\n",
    "\n",
    "## Crafting Grammar Rules\n",
    "\n",
    "The term \"grammar\" in linguistics refers to the set of structural rules governing the composition of clauses, phrases, and words in any given natural language. Grammar is the foundation of every language. These rules determine how words and phrases can be combined to form valid sentences in a language.\n",
    "\n",
    "Grammar encompasses a range of concepts:\n",
    "\n",
    "1) **Syntax:** Deals with the arrangement of words and phrases to create well-formed sentences.\n",
    "2) **Morphology:** Concerns the structure of words, including affixes (prefixes, suffixes), stems, root words, and other morphemes.\n",
    "3) **Phonology:** Studies the way sounds function within a particular language or languages.\n",
    "4) **Semantics:** Examines the meaning of words, phrases, and sentences.\n",
    "5) **Pragmatics:** Analyzes the ways in which context influences the interpretation of meaning.\n",
    "Every language has its own distinct grammar, and while there are universal principles, the specifics can vary widely between languages.\n",
    "\n",
    "\n",
    "In our previous notebook, we focused on establishing rules based on context-free grammar, where we explicitly specified relationships between tokens and their respective tags. While this method provided us with a basic structure to analyze and generate specific phrases, it had limitations, especially when faced with unfamiliar words or varied sentence structures.\n",
    "\n",
    "This time, we'll approach grammar rule creation in a more dynamic and robust manner, accommodating a wider range of sentence structures and lexical variations. \n",
    "\n",
    "Let's formulate these rules. We'll employ a methodology akin to what we explored in the previous notebook, albeit with a distinct approach:\n",
    "\n",
    "- Each line represents a distinct rule.\n",
    "- Establish a rule for 'Who', which denotes the subject.\n",
    "- Define a rule for the action or possession associated with the subject.\n",
    "\n",
    "This method is more efficient because you don't have to tag each individual word as we did in the previous notebook. Instead, you only need to provide rules for the tags  ü•≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r'''\n",
    "Quien: {<nc.*>}\n",
    "Que: {}\n",
    "'''\n",
    "\n",
    "# Then let us create the parser:\n",
    "parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323dde7",
   "metadata": {},
   "source": [
    "The rule 'Who' would be, for instance, for the child. The rule is `<nc.*>`, representing any sequence that starts with 'nc'; it acts like a wildcard. For instance, it is equivalent to `<ncfs000 | ncms000>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af963b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_parser(_tokens):\n",
    "    return parser.parse(_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9855b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'El ni√±o tiene una pelota de baloncesto amarilla'\n",
    "my_parser(hmm.tag(tokenization.tokenize(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270a0eb4",
   "metadata": {},
   "source": [
    "This rule isn't entirely accurate. When we query 'Who', the responses are \"ni√±o\", \"pelota\", and \"baloncesto\". This is because the algorithm categorizes all entries matching 'nc.\\*' under 'Who'. However, the true response for 'Who' should be \"ni√±o\" since it's the subject, being preceded by a '<da0.*>'.\n",
    "\n",
    "**Incorporate this rule!**\n",
    "\n",
    "Where should you place it: before or after the brace ('{')?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af972fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete with your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28545f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'El ni√±o tiene una pelota de baloncesto amarilla'\n",
    "my_parser(hmm.tag(tokenization.tokenize(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5f49aa",
   "metadata": {},
   "source": [
    "**We've now successfully identified the subject of the sentence!**\n",
    "\n",
    "Is everything ok?\n",
    "\n",
    "Are you certain?\n",
    "\n",
    "But what about 'amarilla'? Shouldn't it be labeled as 'aq0fs0'?\n",
    "\n",
    "Mislabeling is a common problem when training algorithms. To address such issues, you'd typically need to retrain your model with the corrected data.\n",
    "\n",
    "**Exercise:** You know the task ahead... Ensure your algorithm doesn't mislabel 'amarilla'.\n",
    "\n",
    "**Warning:**  Always remember that when you're training or retraining, you should focus on entire sentences, not individual words or tokens. Retraining only with 'amarilla' might jeopardize your entire model! üòÆ\n",
    "\n",
    "**Hint:** You don't need to retrain the tagger from scratch! With Hidden Markov Models, you can specifically retrain on mislabeled instances using:\n",
    "\n",
    "`hmm = hmm.train(fix_tag)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125dabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to fix the taggs:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175ccfd",
   "metadata": {},
   "source": [
    "If your code is correct, running the following cell should yield the correct tags  :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'El ni√±o tiene una pelota de baloncesto amarilla'\n",
    "my_parser(hmm.tag(tokenization.tokenize(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd61033",
   "metadata": {},
   "source": [
    "Now let us give rules to the predicative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf81116",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r'''\n",
    "Suj: {<da0.*> <ncfs000 | ncms000>}\n",
    "Pred: <Suj> {<.*>*}\n",
    "'''\n",
    "parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda72069",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'El ni√±o tiene una pelota de baloncesto amarilla'\n",
    "my_parser(hmm.tag(tokenization.tokenize(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c17fd",
   "metadata": {},
   "source": [
    "As previously mentioned multiple times, you can introduce various types of subjects on separate lines in `grammar` (the subject of a sentence can take various forms). However, always ensure they are ordered from the most restrictive to the least restrictive.\n",
    "\n",
    "Let's proceed with the rules inside the predicate, such as the direct object complement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e05d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r'''\n",
    "Suj: {<da0.*> <ncfs000 | ncms000>}\n",
    "DOC: <vmi.*> {<.*>*}\n",
    "Pred: <Suj> {<vmi.*> <DOC>}\n",
    "'''\n",
    "parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'El ni√±o tiene una pelota de baloncesto amarilla'\n",
    "my_parser(hmm.tag(tokenization.tokenize(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9881524",
   "metadata": {},
   "source": [
    "We need to redefine our grammer. Check what happens if tou analyze the phrase \"el abuelo come\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd411fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here:\n",
    "test_phrase = 'El abuelo come'\n",
    "my_parser(hmm.tag(tokenization.tokenize(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01021b68",
   "metadata": {},
   "source": [
    "I will redefine the grammer for you.\n",
    "\n",
    "You are welcome ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r'''\n",
    "Suj: {<da0.*> <ncfs000 | ncms000>}\n",
    "PP: <nc.*> {<sp.*> <nc.*>}\n",
    "DO: <vmi.*> {<di.*> <nc.*> <PP> <aq.*>}\n",
    "DO: <vmi.*> {<di.*> <nc.*>}\n",
    "Pred: <Suj> {<vmi.*> <.*>*}\n",
    "'''\n",
    "parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b006d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = 'El ni√±o come'\n",
    "my_parser(hmm.tag(tokenization.tokenize(test_phrase)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3ffa1",
   "metadata": {},
   "source": [
    "In this manner, in 'El ni√±o come', the predicate is recognized.\n",
    "\n",
    "I've also added the analysis for the Prepositional Phrase (PP) that modifies 'pelota'.\n",
    "\n",
    "You are welcome again!\n",
    "\n",
    "Try how it works!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2822ebd",
   "metadata": {},
   "source": [
    "## Things You Learn Are Not Isolated Islands!\n",
    "\n",
    "Remember, the skills and techniques you learn are not isolated; they're part of a broader toolkit that you can apply in versatile ways. Recall how you've learned to divide datasets into training and testing sets? We'll employ this same foundational concept here in the realm of NLP. \n",
    "\n",
    "Specifically, we'll divide our tagged tokens into two sets. The first set, intended for training, will contain 90% of the tokens. The second set, allocated for testing, will contain the remaining 10%. As we proceed, we'll compare the performance of various taggers, including Unigram, Bigram, Trigram, and the Hidden Markov Model.\n",
    "\n",
    "### Tagger training\n",
    "\n",
    "Before we can effectively train our taggers, we need to understand the dataset we're working with. We'll begin by exploring a well-structured tagging corpus that consists of tokenized phrases from various themes. This foundational understanding will equip us to train our tagger models more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cess_esp.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb962a",
   "metadata": {},
   "source": [
    "As stated before, we will now create two sets of tagged tokens. The first set will contain 80% of the tokens for training, while the second set will comprise the remaining 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Generamos los conjuntos de Train y Test\n",
    "data_train, data_test = train_test_split(cess_esp.tagged_sents(), test_size=0.20, random_state=1)\n",
    "\n",
    "print('Train tokens:',len(data_train),\n",
    "      '\\nTokens test: ',len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefbd1d6",
   "metadata": {},
   "source": [
    "Having the sets already created, we move on to train the taggers.\n",
    "\n",
    "To train the N-grams we must execute the tagger with the corpus, for example `UnigramTagger(data_train)`. We will see that the N-grams can have as backoff another (N-1)-grams.\n",
    "\n",
    "In the case of HiddenMarkovModelTagger we must execute the function `.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram  = UnigramTagger(data_train)\n",
    "bigram   = BigramTagger(data_train, backoff=unigram)\n",
    "trigram  = TrigramTagger(data_train, backoff=bigram)\n",
    "hmm      = HiddenMarkovModelTagger.train(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab689799",
   "metadata": {},
   "source": [
    "Once the taggers are trained, we'll proceed to evaluate their performance using the test set. To accomplish this, we'll utilize the `.accuracy()` function for each of the taggers under test. Let's examine how each performs.\n",
    "\n",
    "When running the testing script, be mindful of the execution time for each tagger. You'll find that while the N-gram taggers are relatively quick at extracting information, the HMM takes longer to process the data. This is one of the reasons why I opted for an 80-20% training-testing split; assessing the accuracy for the HMM tagger was quite time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Acierto con unigramas: %.2f %%' % (unigram.accuracy(data_test)*100))\n",
    "print ('Acierto con bigramas:  %.2f %%' % (bigram.accuracy(data_test)*100))\n",
    "print ('Acierto con trigramas: %.2f %%' % (trigram.accuracy(data_test)*100))\n",
    "print ('Acierto con HMMs:      %.2f %%' % (hmm.accuracy(data_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8569ff",
   "metadata": {},
   "source": [
    "To wrap up this notebook, retrain all the algorithms using the test set and then evaluate their accuracies using the same test set.\n",
    "\n",
    "Upon re-evaluating the taggers on the test set, you'll notice that their performance metrics inch closer to 100% accuracy. This improvement is particularly notable in the N-gram taggers as they operate on more straightforward, 'logical' rules. Therefore, even with small volumes of data (like our test set), they tend to perform remarkably well.\n",
    "\n",
    "In contrast, while the Hidden Markov Model (HMM) may show less dramatic improvement when re-evaluated on the test set, it tends to offer superior performance on unseen data‚Äîtokens that were not part of the training set. This characteristic makes the HMM more robust and versatile in real-world applications where the input data can be highly variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12303c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
