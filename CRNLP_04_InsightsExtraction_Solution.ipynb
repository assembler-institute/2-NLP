{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524d7e0b",
   "metadata": {},
   "source": [
    "# NLP IV: Information Extraction in NLP\n",
    "\n",
    "In this notebook, we will delve into the topics of labeling, label recognition, and information extraction within the domain of natural language processing. By understanding these concepts, we'll be better equipped to handle and utilize the information efficiently.\n",
    "\n",
    "## Data labeling and recognition\n",
    "We've already covered the process of labeling data in our previous notebook. Now, let's begin by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667e605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "from nltk.chunk.regexp import *\n",
    "\n",
    "# Do't forget to import your library.\n",
    "# Type your code here:\n",
    "from Libraries import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d683e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Eustaquio tiene un coche amarillo',\n",
    "    'Hermenegildo tiene un mechero',\n",
    "    'Sole tiene un huevo de avestruz',\n",
    "    'María quiere una tele curva',\n",
    "    'El niño come una tortilla de patatas',\n",
    "    'El niño alto juega al baloncesto'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a010e3b",
   "metadata": {},
   "source": [
    "Once we have a corpus of text at our disposal, the initial step in processing this data is to tokenize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb7c9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eustaquio', 'tiene', 'un', 'coche', 'amarillo'],\n",
       " ['hermenegildo', 'tiene', 'un', 'mechero'],\n",
       " ['sole', 'tiene', 'un', 'huevo', 'de', 'avestruz'],\n",
       " ['maría', 'quiere', 'una', 'tele', 'curva'],\n",
       " ['el', 'niño', 'come', 'una', 'tortilla', 'de', 'patatas'],\n",
       " ['el', 'niño', 'alto', 'juega', 'al', 'baloncesto']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_phrases = []\n",
    "\n",
    "for phrase in corpus:\n",
    "    token_phrases.append(tokenization.tokenize(phrase))\n",
    "    \n",
    "token_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7cbdfa",
   "metadata": {},
   "source": [
    "The next step is to create and train the taggers. To do this, let's import the cess_esp corpus. This will provide us with a substantial training set, allowing us to train the Hidden Markov Model (HMM) effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea54426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "\n",
    "hmm = HiddenMarkovModelTagger.train(cess_esp.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f8f2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('eustaquio', 'sn.e-SUJ'),\n",
       "  ('tiene', 'vmip3s0'),\n",
       "  ('un', 'di0ms0'),\n",
       "  ('coche', 'ncms000'),\n",
       "  ('amarillo', 'aq0ms0')],\n",
       " [('hermenegildo', 'sn.e-SUJ'),\n",
       "  ('tiene', 'vmip3s0'),\n",
       "  ('un', 'di0ms0'),\n",
       "  ('mechero', 'ncms000')],\n",
       " [('sole', 'sn.e-SUJ'),\n",
       "  ('tiene', 'vmip3s0'),\n",
       "  ('un', 'di0ms0'),\n",
       "  ('huevo', 'ncms000'),\n",
       "  ('de', 'sps00'),\n",
       "  ('avestruz', 'da0fs0')],\n",
       " [('maría', 'sn.e-SUJ'),\n",
       "  ('quiere', 'vmip3s0'),\n",
       "  ('una', 'di0fs0'),\n",
       "  ('tele', 'ncfs000'),\n",
       "  ('curva', 'aq0fs0')],\n",
       " [('el', 'da0ms0'),\n",
       "  ('niño', 'ncms000'),\n",
       "  ('come', 'vmip3s0'),\n",
       "  ('una', 'di0fs0'),\n",
       "  ('tortilla', 'ncfs000'),\n",
       "  ('de', 'sps00'),\n",
       "  ('patatas', 'ncfp000')],\n",
       " [('el', 'da0ms0'),\n",
       "  ('niño', 'ncms000'),\n",
       "  ('alto', 'aq0ms0'),\n",
       "  ('juega', 'vmip3s0'),\n",
       "  ('al', 'spcms'),\n",
       "  ('baloncesto', 'ncms000')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_phrases = []\n",
    "for phrase in token_phrases:\n",
    "    tagged_phrases.append(hmm.tag(phrase))\n",
    "tagged_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb10232",
   "metadata": {},
   "source": [
    "As you can see, the algorithm struggles with accurately labeling proper names. This issue can often be addressed by supplementing the training data. For instance, adding the label `('eustaquio', 'np00000')` could help the algorithm correctly identify 'Eustaquio' as a proper noun. Usually, a single additional label like this would suffice to correctly tag other proper names. However, because our algorithm was initially trained on a relatively small dataset containing just 6030 sentences, one small change can have unintended side effects. For example, after retraining with this new label, you might find that common nouns like 'ostrich' start being incorrectly identified as proper names, primarily because they were not in the original training set.\n",
    "\n",
    "At this juncture, it might be tempting to correct these errors, but it's essential to remember our broader learning objectives. We are still in the initial stages of understanding the underpinnings of this field. In a real-world application, these algorithms are trained on much larger datasets, and any mislabeling is manually corrected. The benefit of using more extensive training data is that individual corrections are less likely to disrupt other components that are already functioning correctly.\n",
    "\n",
    "So, let's proceed without retraining using the `cess_esp.tagged_sents()` dataset, and continue working with the 'sn.e-SUJ' label for proper names (which is the label trained in our dataset). While this is not an ideal approach, it prevents our algorithm from becoming confused when we make corrections to the tags.\n",
    "\n",
    "Now let's move on to the grammatical rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80a7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining the code with this script (we skip this step):\n",
    "\n",
    "# manual_train = [\n",
    "#     [('sole', 'np00000'),\n",
    "#         ('tiene', 'vmip3s0'),\n",
    "#         ('un', 'di0ms0'),\n",
    "#         ('huevo', 'ncms000'),\n",
    "#         ('de', 'sps00'),\n",
    "#         ('avestruz', 'ncms000'),\n",
    "#         ('amarillo', 'aq0ms0')],\n",
    "#     [('maría', 'np00000'),\n",
    "#         ('quiere', 'vmip3s0'),\n",
    "#         ('una', 'di0fs0'),\n",
    "#         ('tele', 'ncfs000'),\n",
    "#         ('curva', 'aq0fs0')]\n",
    "#     ]\n",
    "# hmm = hmm.train(manual_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b85821b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer = r'''\n",
    "Who: {<sn.e-SUJ>}\n",
    "Who: {<da.*> <nc.*> <aq.*> } <vm.*>\n",
    "Who: {<da.*> <nc.*>} <vm.*>\n",
    "What: <vm.*> {<.*>*}\n",
    "Acc: {<vm.*>}\n",
    "'''\n",
    "\n",
    "parser = nltk.RegexpParser(grammer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0848ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_parser(_tokens):\n",
    "    return parser.parse(_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "772217f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,424.0,168.0\" width=\"424px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"47.1698%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Who</text></svg><svg width=\"32%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">el</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">da0ms0</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"36%\" x=\"32%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">niño</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ncms000</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"32%\" x=\"68%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">alto</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">aq0ms0</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.5849%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"16.9811%\" x=\"47.1698%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Acc</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">juega</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">vmip3s0</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.6604%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"35.8491%\" x=\"64.1509%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">What</text></svg><svg width=\"36.8421%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">al</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">spcms</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.4211%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"63.1579%\" x=\"36.8421%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">baloncesto</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ncms000</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.4211%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.0755%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('Who', [('el', 'da0ms0'), ('niño', 'ncms000'), ('alto', 'aq0ms0')]), Tree('Acc', [('juega', 'vmip3s0')]), Tree('What', [('al', 'spcms'), ('baloncesto', 'ncms000')])])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_parser(tagged_phrases[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cc2d8",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "\n",
    "Next, we will analyze each node in the syntactic tree to extract valuable information.\n",
    "\n",
    "This section aims to provide an understanding of how to navigate through a syntactic tree for the purpose of information extraction. Whether you are looking to identify subjects, objects, or other syntactic elements, understanding the structure of these trees is crucial.\n",
    "\n",
    "By examining each node, we can pull out various pieces of data that could be beneficial for tasks like text summarization, named entity recognition, or even sentiment analysis.\n",
    "\n",
    "Ready to dive in? Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7482954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(_tree):\n",
    "    \n",
    "    result = {\"Who\": '', \"Action\": None, \"Obj\": ''} \n",
    "    \n",
    "    for node in _tree:\n",
    "        \n",
    "        print('Node:', node)\n",
    "\n",
    "        if type(node) != tuple: # To avoid nodes without labeling\n",
    "            \n",
    "            # Let us detect each label:\n",
    "\n",
    "            if node.label() == 'Who':\n",
    "                for element in node:\n",
    "                    word, tag = element\n",
    "                    result['Who'] = result['Who'] + ' ' + word\n",
    "                    \n",
    "            elif node.label() == 'Acc':\n",
    "                word, tag = node[0]\n",
    "                result['Action'] = word\n",
    "                \n",
    "            elif node.label() == 'What':\n",
    "                \n",
    "                for element in node:\n",
    "                    word, tag = element\n",
    "                    result['Obj'] = result['Obj'] + ' ' + word\n",
    "                    \n",
    "    result['Obj'] = result['Obj'].strip()\n",
    "    result['Who'] = result['Who'].strip()\n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b15d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: (Who hermenegildo/sn.e-SUJ)\n",
      "Node: (Acc tiene/vmip3s0)\n",
      "Node: (What un/di0ms0 mechero/ncms000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Who': 'hermenegildo', 'Action': 'tiene', 'Obj': 'un mechero'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract(my_parser(tagged_phrases[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ff759",
   "metadata": {},
   "source": [
    "In what follows, let us introduce an alternative, more straightforward approach for extracting information. Please note that this simplified script is best suited for basic taggers. While this approach is easier to implement, it may not capture all the nuances that a more sophisticated method would. So, it's a trade-off between simplicity and depth of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e66554bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_extraction(_tree):\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for node in _tree:\n",
    "        \n",
    "        if type(node) != tuple:\n",
    "            \n",
    "            for element in node:\n",
    "                word, tag = element\n",
    "                try:\n",
    "                    result[node.label()] = result[node.label()] + ' ' + word\n",
    "                except:\n",
    "                    result[node.label()] = word\n",
    "                    \n",
    "    for key in result.keys():\n",
    "        result[key] = result[key].strip()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c59fa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Who': 'maría', 'Acc': 'quiere', 'What': 'una tele curva'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis = my_extraction(my_parser(tagged_phrases[3]))\n",
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5201378",
   "metadata": {},
   "source": [
    "If sentences are simple, this script provides a highly automated way of extracting information. Learn from it to simplify your code!\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "Now that we have successfully extracted the data, let's move on to the exciting part—data analysis! This is where we can uncover insights, identify patterns, and perhaps even make predictions based on the data we have gathered. Stay tuned as we delve deeper into various techniques and tools that will help you understand and analyze your data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1968fccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Hola,\n",
      "\n",
      "        maría, nos ha dicho que quiere una tele curva.\n",
      "\n",
      "        No puedo más con este cliente, ¿Alquien sabe dónde conseguir una tele curva?\n",
      "\n",
      "        Saludos\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if analysis['Acc'] == 'tiene':\n",
    "    output = f\"\"\"\n",
    "        Hola,\n",
    "\n",
    "        {analysis['Who']}, nos ha dicho que {analysis['Acc']} {analysis['What']}.\n",
    "\n",
    "        ¿Como le robamos {analysis['What']}?\n",
    "\n",
    "        Saludos\n",
    "    \"\"\"\n",
    "    \n",
    "elif analysis['Acc'] == 'quiere':\n",
    "    output = f\"\"\"\n",
    "        Hola,\n",
    "\n",
    "        {analysis['Who']}, nos ha dicho que {analysis['Acc']} {analysis['What']}.\n",
    "\n",
    "        No puedo más con este cliente, ¿Alquien sabe dónde conseguir {analysis['What']}?\n",
    "\n",
    "        Saludos\n",
    "    \"\"\"\n",
    "    \n",
    "elif analysis['Acc'] == 'come':\n",
    "    output = f\"\"\"\n",
    "        Hola,\n",
    "\n",
    "        {analysis['Who']}, nos ha dicho que prefiere alimentarse a base de {analisis['What']}.\n",
    "\n",
    "        ¿{analysis['Who']} es vegano?\n",
    "\n",
    "        Saludos\n",
    "    \"\"\"\n",
    "    \n",
    "else:\n",
    "    output = f\"\"\"\n",
    "        Hola,\n",
    "\n",
    "        {analisis['Who']} es una pesadilla, me lo quiero quitar de encima.\n",
    "        \n",
    "        ¿Alguna sugerencia para cargarnoslo mientras le engañamos {analysis['Acc']} {analysis['What']}?\n",
    "\n",
    "        Saludos\n",
    "    \"\"\"\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3276f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It may appear simple on the surface, but you can imagine the endless applications you can derive from this. For instance, a user could input information into a chatbot, which then provides a tailored output based on that user's specific data. The takeaway here, dear budding data scientists, is that it's all about transforming information to extract value from it. Simple yet powerful process is the cornerstone of what we aim to achieve in data science.!\n",
    "\n",
    "And the possibilities don't stop there! You could even query a database based on the user's input as a filter to provide even more personalized responses. The sky is the limit when it comes to leveraging data for meaningful interactions and solutions. 🤓\n",
    "\n",
    "Thank you for following along in these notebooks. As you continue your journey in data science and natural language processing, remember that the key is to always find ways to add value through data. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Happy coding!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac48998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM catalogue WHERE product_title like %'una tele curva'%\n"
     ]
    }
   ],
   "source": [
    "sql = \"SELECT * FROM catalogue WHERE product_title like %'\" + analysis['What'] + \"'%\"\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c12fd9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM clients WHERE name like %'maría'%\n"
     ]
    }
   ],
   "source": [
    "sql = \"SELECT * FROM clients WHERE name like %'\" + analysis['Who'] + \"'%\"\n",
    "print(sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
