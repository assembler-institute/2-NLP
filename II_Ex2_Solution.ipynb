{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP II - Exercise 2 solution\n",
    "\n",
    "A major restaurant chain wants to implement a chat ordering system, so that customers simply write their order on a tablet or via whatsapp and the virtual assistant will take care of managing the order with the kitchen and waiters.\n",
    "\n",
    "The restaurant needs to have a list of the dishes and drinks separately along with the number of units of each item ordered (and the ingredients as optional).\n",
    "\n",
    "To do this exercise, train and compare the performance of each tagger, then choose the best one. You will have to retrain your tagger if needed.\n",
    "\n",
    "## Libraries\n",
    "\n",
    "We will work with the NLTK library, with the tokeniser included in the library, the cess_esp corpus and the 4 taggers seen in the unit. We will also have to import the NLTK RegEx Parser.\n",
    "\n",
    "Finally, as non-mandatory add-ons that we will use to format or make the work easier, sklearn's train_test_split and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#We import the NLTK component to tokenise\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#We import the CESS corpus in Spanish\n",
    "from nltk.corpus import cess_esp\n",
    "\n",
    "#Taggers ngrams y HMM\n",
    "from nltk import UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "#RegEx Parser\n",
    "from nltk.chunk.regexp import *\n",
    "\n",
    "\n",
    "#This will allow us to create the test and train sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Finally we import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger training\n",
    "\n",
    "To train the taggers, what we have to do is to get the tagging corpus in English. First, let's see what the corpus contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cess_esp.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')], [('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explicó', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcción', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prevé', 'vmm02s0'), ('la', 'da0fs0'), ('utilización', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with the command `cess_esp.sents()` we get a set of tokenised phrases of different themes.\n",
    "\n",
    "And with the command `cess_esp.tagged_sents()` we get the same set of tagged sentences.\n",
    "\n",
    "Now we will create 2 sets of tagged tokens. One containing 90% of the tokens and the other containing 10%, one for train and one for test respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 5427 \n",
      "Tokens test:     603\n"
     ]
    }
   ],
   "source": [
    "#We generate the Train and Test sets\n",
    "data_train, data_test = train_test_split(cess_esp.tagged_sents(), test_size=0.10, random_state=1)\n",
    "\n",
    "print('Train tokens:',len(data_train),\n",
    "      '\\nTokens test:    ',len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the sets already created, we move on to train the taggers.\n",
    "\n",
    "To train the ngrams we must execute the tagger with the corpus, for example UnigramTagger(data_train). We will see that the ngrams can have as backoff another ngram.\n",
    "\n",
    "In the case of HiddenMarkovModelTagger we must execute the function .train().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram  = UnigramTagger(data_train)\n",
    "bigram   = BigramTagger(data_train, backoff=unigram)\n",
    "trigram  = TrigramTagger(data_train, backoff=bigram)\n",
    "hmm      = HiddenMarkovModelTagger.train(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the taggers have been trained, we are going to evaluate how each of them tends to perform with the test set. To evaluate it we have to use the train() function, for all the taggers. Let's see how each of them performs.\n",
    "\n",
    "When you run the training, pay attention to the time it takes for each of the taggers to display the score. While the ngrams are quite fast to extract the information, the HMM takes longer to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_776023/3977873343.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print ('Hit with unigramas: %.2f %%' % (unigram.evaluate(data_test)*100))\n",
      "/tmp/ipykernel_776023/3977873343.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print ('Hit with bigramas:  %.2f %%' % (bigram.evaluate(data_test)*100))\n",
      "/tmp/ipykernel_776023/3977873343.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print ('Hit with trigramas: %.2f %%' % (trigram.evaluate(data_test)*100))\n",
      "/tmp/ipykernel_776023/3977873343.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print ('Hit with HMMs:      %.2f %%' % (hmm.evaluate(data_test)*100))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit with unigramas: 87.27 %\n",
      "Hit with bigramas:  88.78 %\n",
      "Hit with trigramas: 88.77 %\n",
      "Hit with HMMs:      89.57 %\n"
     ]
    }
   ],
   "source": [
    "print ('Hit with unigramas: %.2f %%' % (unigram.evaluate(data_test)*100))\n",
    "print ('Hit with bigramas:  %.2f %%' % (bigram.evaluate(data_test)*100))\n",
    "print ('Hit with trigramas: %.2f %%' % (trigram.evaluate(data_test)*100))\n",
    "print ('Hit with HMMs:      %.2f %%' % (hmm.evaluate(data_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can retrain the taggers with the test data. Although we will not see a big improvement in general terms, as the volume of data we are using is small, it will help.\n",
    "\n",
    "We will see that if we evaluate the taggers again on the test set, we will get almost 100% accuracy. This improvement is greater in the ngrams because they are more 'logical' rules, so to speak, so with small volumes of data we will see more success. On the other hand, in the HMM, we see better improvement in the case of evaluating on the data but in those tokens not trained we will have better performance than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto con unigramas: 96.67 %\n",
      "Acierto con bigramas:  98.86 %\n",
      "Acierto con trigramas: 99.56 %\n",
      "Acierto con HMMs:      93.62 %\n"
     ]
    }
   ],
   "source": [
    "unigram  = UnigramTagger(data_test)\n",
    "bigram   = BigramTagger(data_test, backoff=unigram)\n",
    "trigram  = TrigramTagger(data_test, backoff=bigram)\n",
    "hmm      = HiddenMarkovModelTagger.train(data_test)\n",
    "\n",
    "print ('Acierto con unigramas: %.2f %%' % (unigram.evaluate(data_test)*100))\n",
    "print ('Acierto con bigramas:  %.2f %%' % (bigram.evaluate(data_test)*100))\n",
    "print ('Acierto con trigramas: %.2f %%' % (trigram.evaluate(data_test)*100))\n",
    "print ('Acierto con HMMs:      %.2f %%' % (hmm.evaluate(data_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen that the HMM has better performance, a priori, we will use this one to elaborate our command bot.\n",
    "\n",
    "## Start building the bot\n",
    "\n",
    "Now we will create a sentence of our bot's theme to see how it performs with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_text = 'Quiero unos macarrones con queso y una cerveza'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to work out the sentence tokens. For the taggers it is not necessary to eliminate stopwords, lemmatisation or derivation, on the contrary. If we do all these steps we will be eliminating information that the taggers will use to find the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quiero', 'unos', 'macarrones', 'con', 'queso', 'y', 'una', 'cerveza']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(food_text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the tokens, we will use the `.tag()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quiero', 'da0mp0'),\n",
       " ('unos', 'di0mp0'),\n",
       " ('macarrones', 'ncmp000'),\n",
       " ('con', 'sps00'),\n",
       " ('queso', 'np0000l'),\n",
       " ('y', 'cc'),\n",
       " ('una', 'di0fs0'),\n",
       " ('cerveza', 'ncfs000')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_tagged = hmm.tag(tokens)\n",
    "food_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tags obtained are not quite correct. To check them, we should check the EAGLES tags in the link provided in the syllabus: https://www.cs.upc.edu/~nlp/tools/parole-sp.html.\n",
    "\n",
    "In summary, we will use these tags, although we can have variations and incorporate others;\n",
    "\n",
    "- **ncms000** : nombre común masculino singular\n",
    "- **ncfs000** : nombre común femenino singular\n",
    "- **ncmp000** : nombre común masculino plural\n",
    "- **ncfp000** : nombre común femenino plural\n",
    "\n",
    "- **np0000p/np00001** : nombre propio (We include it because if we look at the way our examples are labelled we can see that some words are labelled in this way (e.g.: tuna sandwich). It could also be the case that some food follows this structure correctly (e.g. Toni's pizza).\n",
    "\n",
    "\n",
    "- **di0ms0** : determinante indefinido masculino singular\n",
    "- **di0fs0** : determinante indefinido femenino singular\n",
    "- **di0mp0** : determinante indefinido masculino plural\n",
    "- **di0fp0** : determinante indefinido femenino plural\n",
    "- **dn0cp0** : determinante indefinido comun plural\n",
    "\n",
    "\n",
    "- **sps00** : Preposición\n",
    "\n",
    "\n",
    "- **da0ms0**: el\n",
    "- **da0fs0**: la\n",
    "- **da0mp0**: los\n",
    "- **da0fp0**: las\n",
    "- **da0ns0**: lo\n",
    "\n",
    "Let's review the sentence:\n",
    "\n",
    " * ('Quiero', 'da0mp0') -> Taggeado como un determinante, debería ser: vmpip1s0 que corresponde a presente de indicativo\n",
    " * ('unos', 'di0mp0') -> Taggeado como determinante masculino, debería ser: mcmp00 que corresponde a un numeral ordinal masculino\n",
    " * ('macarrones', 'ncmp000'), -> **Correcto**: Taggeado como Sustantivo Común Masculino Plural\n",
    " * ('con', 'sps00'), -> **Correcto**: Taggeado como preposición\n",
    " * ('queso', 'np0000l'), -> Taggeado como Sustantivo Propio, debería ser: ncms000 sustantivo común masculino singular\n",
    " * ('y', 'cc'),-> **Correcto**: Taggeado como conjunción coordinada\n",
    " * ('una', 'di0fs0'),-> Taggeado como determinante femenino, debería ser: mcfp00 que corresponde a un numeral ordinal femenino\n",
    " * ('cerveza', 'ncfs000')-> -> **Correcto**: Taggeado como sustantivo femenino singular\n",
    " \n",
    " As we have seen, correctly tagged we would have 4 tokens out of 8, a 50% hit rate. If we evaluate these tags with the performance of, for example, the trigram, what % of hits will we have? Let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acierto con unigramas: 50.00 %\n",
      "Acierto con bigramas:  50.00 %\n",
      "Acierto con trigramas: 50.00 %\n",
      "Acierto con HMMs:      100.00 %\n"
     ]
    }
   ],
   "source": [
    "print ('Acierto con unigramas: %.2f %%' % (unigram.evaluate([food_tagged])*100))\n",
    "print ('Acierto con bigramas:  %.2f %%' % (bigram.evaluate([food_tagged])*100))\n",
    "print ('Acierto con trigramas: %.2f %%' % (trigram.evaluate([food_tagged])*100))\n",
    "print ('Acierto con HMMs:      %.2f %%' % (hmm.evaluate([food_tagged])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, HMM says that it is 100% correct, which is logical since it is this tagger that has made the tags. On the other hand, the rest of the tags coincide in the correction we have made, although this does not mean that these tags are correct 8 out of 8 tags, let's check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigramas: [('Quiero', None), ('unos', 'di0mp0'), ('macarrones', None), ('con', 'sps00'), ('queso', None), ('y', 'cc'), ('una', 'di0fs0'), ('cerveza', None)]\n",
      "Bigramas:  [('Quiero', None), ('unos', 'di0mp0'), ('macarrones', None), ('con', 'sps00'), ('queso', None), ('y', 'cc'), ('una', 'di0fs0'), ('cerveza', None)]\n",
      "Trigramas:  [('Quiero', None), ('unos', 'di0mp0'), ('macarrones', None), ('con', 'sps00'), ('queso', None), ('y', 'cc'), ('una', 'di0fs0'), ('cerveza', None)]\n"
     ]
    }
   ],
   "source": [
    "print ('Unigramas:', (unigram.tag(tokens)))\n",
    "print ('Bigramas: ', (bigram.tag(tokens)))\n",
    "print ('Trigramas: ', (trigram.tag(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen these results, we confirm that the best performer is HMM, which has found labels, which although not entirely correct have been approximate, in some cases it has confused the gender, in others the number, although this would not affect our information extraction too much. But in general it has tagged 6 of the 8 tokens in word type.\n",
    "\n",
    "The ngrams, however, failed to identify the tokens in 50% of the cases. So we would have had problems when using them as taggers.\n",
    "\n",
    "## Correct the tagger\n",
    "\n",
    "Now that we've corrected the tags, it's time to retrain the tagger with the correct phrase, so let's get on with it.\n",
    "We're also going to work with the foodTagger (it's an HMM trained with our food phrases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_tokens = [('Quiero', 'vmpip1s0'), ('unos', 'mcmp00'), ('macarrones', 'ncmp000'), ('con', 'sps00'), ('queso', 'ncms000'), ('y', 'cc'), ('una', 'mcfp00'), ('cerveza', 'ncfs000')]\n",
    "\n",
    "foodTagger = hmm.train([corrected_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once the tagger is trained, if we pass the same phrase again, it will hit all of them because it is trained to detect the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quiero', 'vmpip1s0'),\n",
       " ('unos', 'mcmp00'),\n",
       " ('macarrones', 'ncmp000'),\n",
       " ('con', 'sps00'),\n",
       " ('queso', 'ncms000'),\n",
       " ('y', 'cc'),\n",
       " ('una', 'mcfp00'),\n",
       " ('cerveza', 'ncfs000')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_tagged = foodTagger.tag(tokens)\n",
    "food_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correction of the tagger is a process that we will have to elaborate repeatedly in order to improve the results. It is a process that could be greatly shortened if we had a corpus with thousands of phrases and tokens like the initial one, but from our specific context.\n",
    "\n",
    "## Develop a function to recognise orders\n",
    "\n",
    "For this we will have to use RegEx Parser and the logical rules. In our case, we can use the ones we have seen in the theory of this unit.\n",
    "\n",
    "- nombre común : *macarrones*\n",
    "\n",
    "- nombre común + nombre (común/propio) : *pizza margarita*\n",
    "\n",
    "- nombre común + preposición + nombre(común/propio) : *bocadillo de atún*\n",
    "\n",
    "- nombre común + preposición + artículo + nombre(común/propio) : *lentejas a la riojana*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reglas = r'''\n",
    "    cantidad: {<mccp00>}\n",
    "    comida: {<ncms000|ncfs000|ncmp000|ncfp000>*<sps00>*<da0ms0|da0fs0|da0mp0|da0fp0|da0ns0>*<ncms000|ncfs000|ncmp000|ncfp000|np0000l|np0000p>}\n",
    "    cantidad: {<di0ms0|di0fs0|di0mp0|di0fp0|dn0cp0|mcmp00|mcfp00> || <mcmp00>* || <mcfp00> }\n",
    "      '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the grammar created, let's create the regex function that extracts the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegexP = nltk.RegexpParser(reglas)\n",
    "\n",
    "def parsear(phrase):\n",
    "    return RegexP.parse(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Quiero/vmpip1s0\n",
      "  (cantidad unos/mcmp00)\n",
      "  (comida macarrones/ncmp000 con/sps00 queso/ncms000)\n",
      "  y/cc\n",
      "  (cantidad una/mcfp00)\n",
      "  (comida cerveza/ncfs000))\n"
     ]
    }
   ],
   "source": [
    "frase_regex = parsear(corrected_tokens)\n",
    "print(frase_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract the classified nodes\n",
    "\n",
    "Once we have managed to identify the food and the quantity of the order, we will generate a JSON with the order data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_comanda(tree):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    item = {}\n",
    "    item['item'] = None\n",
    "    item['cantidad'] = 0\n",
    "    \n",
    "    elementos = 0\n",
    "    \n",
    "    #En primer lugar contaremos cuantos elementos hay en el pedido\n",
    "    for nodo in tree:\n",
    "        if type(nodo) == tuple:\n",
    "            continue\n",
    "        tipo = nodo.label()\n",
    "        if tipo == 'comida':\n",
    "            elementos += 1\n",
    "            \n",
    "    #Ahora generaremos cada línea de pedido con sus cantidades\n",
    "    for nodo in tree:\n",
    "        if type(nodo) == tuple:\n",
    "            continue\n",
    "        \n",
    "        count = 0\n",
    "        valor = ''\n",
    "        \n",
    "        for elemento in nodo:\n",
    "            count += 1\n",
    "            palabra, categoria = elemento\n",
    "                \n",
    "            if count == 1:\n",
    "                valor = valor + palabra\n",
    "            else:\n",
    "                valor = valor + ' ' + palabra\n",
    "            \n",
    "            if nodo.label() == 'cantidad':\n",
    "                item['cantidad'] = valor\n",
    "            else:\n",
    "                item['item'] = valor\n",
    "        \n",
    "        if nodo.label() == 'comida':\n",
    "            result.append(item)\n",
    "            item = {}\n",
    "            #print(item)\n",
    "        \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'item': 'macarrones con queso', 'cantidad': 'unos'},\n",
       " {'cantidad': 'una', 'item': 'cerveza'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genera_comanda(frase_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the function generates the comma, we will generate a new function that does it from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesa_frase(frase):\n",
    "    \n",
    "    tokens = nltk.word_tokenize(frase)\n",
    "    print('Tokens:')\n",
    "    print(tokens)\n",
    "    print('\\n', '-----------------------------------------', '\\n')\n",
    "    tags = foodTagger.tag(tokens)\n",
    "    print('TAGS:')\n",
    "    print(tags)\n",
    "    print('\\n', '-----------------------------------------', '\\n')\n",
    "    parsed = parsear(tags)\n",
    "    print('Parsed:')\n",
    "    print(parsed)\n",
    "    print('\\n', '-----------------------------------------', '\\n')\n",
    "    \n",
    "    return genera_comanda(parsed)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we test the function we have created to parse complete sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['pedir', 'dos', 'pizzas', 'cuatro', 'quesos', 'y', 'cinco', 'fantas']\n",
      "\n",
      " ----------------------------------------- \n",
      "\n",
      "TAGS:\n",
      "[('pedir', 'vmpip1s0'), ('dos', 'mcmp00'), ('pizzas', 'ncmp000'), ('cuatro', 'sps00'), ('quesos', 'ncms000'), ('y', 'cc'), ('cinco', 'mcfp00'), ('fantas', 'ncfs000')]\n",
      "\n",
      " ----------------------------------------- \n",
      "\n",
      "Parsed:\n",
      "(S\n",
      "  pedir/vmpip1s0\n",
      "  (cantidad dos/mcmp00)\n",
      "  (comida pizzas/ncmp000 cuatro/sps00 quesos/ncms000)\n",
      "  y/cc\n",
      "  (cantidad cinco/mcfp00)\n",
      "  (comida fantas/ncfs000))\n",
      "\n",
      " ----------------------------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'item': 'pizzas cuatro quesos', 'cantidad': 'dos'},\n",
       " {'cantidad': 'cinco', 'item': 'fantas'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraseTest = 'pedir dos pizzas cuatro quesos y cinco fantas'\n",
    "\n",
    "procesa_frase(fraseTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this sentence works, others may not, for example, if we don't put a verb before the command, let's do a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['dos', 'pizzas', 'cuatro', 'quesos', 'y', 'cinco', 'fantas']\n",
      "\n",
      " ----------------------------------------- \n",
      "\n",
      "TAGS:\n",
      "[('dos', 'vmpip1s0'), ('pizzas', 'mcmp00'), ('cuatro', 'ncmp000'), ('quesos', 'sps00'), ('y', 'ncms000'), ('cinco', 'cc'), ('fantas', 'mcfp00')]\n",
      "\n",
      " ----------------------------------------- \n",
      "\n",
      "Parsed:\n",
      "(S\n",
      "  dos/vmpip1s0\n",
      "  (cantidad pizzas/mcmp00)\n",
      "  (comida cuatro/ncmp000 quesos/sps00 y/ncms000)\n",
      "  cinco/cc\n",
      "  (cantidad fantas/mcfp00))\n",
      "\n",
      " ----------------------------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'item': 'cuatro quesos y', 'cantidad': 'pizzas'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraseTest = 'dos pizzas cuatro quesos y cinco fantas'\n",
    "procesa_frase(fraseTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sentence you have only identified, and wrongly, so let's correct it and train our tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_order = [('dos', 'mccp00'), ('pizzas', 'ncmp000'), ('cuatro', 'sps00'), ('quesos', 'ncms000'), ('y', 'cc'), ('cinco', 'mcfp00'), ('fantas', 'ncfs000')]\n",
    "\n",
    "foodTagger = foodTagger.train([corrected_order])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now re-trained our tagger, so we can try the same phrase again and check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['dos', 'pizzas', 'cuatro', 'quesos', 'y', 'cinco', 'fantas']\n",
      "\n",
      " ----------------------------------------- \n",
      "\n",
      "TAGS:\n",
      "[('dos', 'mccp00'), ('pizzas', 'ncmp000'), ('cuatro', 'sps00'), ('quesos', 'ncms000'), ('y', 'cc'), ('cinco', 'mcfp00'), ('fantas', 'ncfs000')]\n",
      "\n",
      " ----------------------------------------- \n",
      "\n",
      "Parsed:\n",
      "(S\n",
      "  (cantidad dos/mccp00)\n",
      "  (comida pizzas/ncmp000 cuatro/sps00 quesos/ncms000)\n",
      "  y/cc\n",
      "  (cantidad cinco/mcfp00)\n",
      "  (comida fantas/ncfs000))\n",
      "\n",
      " ----------------------------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'item': 'pizzas cuatro quesos', 'cantidad': 'dos'},\n",
       " {'cantidad': 'cinco', 'item': 'fantas'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procesa_frase(fraseTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['dos', 'hamburguesas']\n",
      "\n",
      " ----------------------------------------- \n",
      "\n",
      "TAGS:\n",
      "[('dos', 'mccp00'), ('hamburguesas', 'ncmp000')]\n",
      "\n",
      " ----------------------------------------- \n",
      "\n",
      "Parsed:\n",
      "(S (cantidad dos/mccp00) (comida hamburguesas/ncmp000))\n",
      "\n",
      " ----------------------------------------- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'item': 'hamburguesas', 'cantidad': 'dos'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procesa_frase('dos hamburguesas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this we would have our command processor bot ready. Of course it has room for improvement and needs further training, but it has the required functionality. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
